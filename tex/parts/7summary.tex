\chapter{Summary}

Recent advances in machine learning, neural network software frameworks, and \ac{GPU} computing capacity have made it possible to design and teach very deep neural networks. \acfp{CNN} have been long used for image processing, and lately, it has been shown that they can be adapted to human face detection and extraction of relevant low-dimensional facial data. An extension of a \ac{CNN}, the \acf{FCNN}, has been recently shown to be capable of doing dense pixel-to-pixel mappings. This mapping has been used, for example, to do dense semantic segmentations of images. To train \acp{FCNN} in a supervised way, a large number of annotated input/output image pairs are needed. Nowadays, because of abundant processing and storage capacity in computing clusters, it is feasible to generate these kinds of input/output image pairs synthetically using rendering software. Also, data augmentation has been shown to expand the effective size of even small training datasets successfully.

Because the \ac{FCNN} is an extension to the \ac{CNN}, it is possible to train the former to do dense mapping of human faces. The downscaling part of the \ac{FCNN} first detects the relevant features of the face and compresses them down to a lower dimensional presentation. The upscaling part of the \ac{FCNN} then uses the representation to generate a segmentation and a dense mapping of the human face. One problem is to find a dense mapping that would represent the geometry of the face and that could be taught to a neural network. Another problem is to find training data of real-world images with the aforementioned mapping. Creating the mappings by hand would be very time-consuming as at least tens of thousands of training samples would be needed. On the contrary, generating a training dataset like this by rendering would be easy, as access to exact underlying geometry is available.

The main research goal of the thesis was to explore whether it is possible to train an \ac{FCNN} using non-realistic synthetic data to do dense facial geometry tracking of real-world human faces. Subgoals included designing a method to generate the training data, making the network inpaint geometry under occlusions, making the network temporally stable when applied to video, and designing visualization methods for the generated mappings to ease the evaluation of the results.

To generate the synthetic training dataset, we used Blender to render 100 000 training samples of a randomized head mesh. One sample included an input image, a UV image, and a mask image. The input image had realistic backgrounds but non-realistic face materials. We decided to use the underlying UV mapping of the head model to render the UV image, which then represented the geometry mapping in our method. The mask image was used to segment the frontal face area out of the input image.

We based the design of our \ac{FCNN} on the U-net network topology. Seven levels were used and skip connections were added between all corresponding down and upsampling parts. Convolution feature map sizes, convolution filter sizes, and activation functions were slightly adjusted to optimize results. In the end, as a loss function, we used a simple sum of L1 losses between UV, UV gradient and mask images. UV image gradients were used to speed converge and enforce the smoothness of the resulting UV mapping.

Data augmentation was implemented by applying the following to the input image: rotations, color channels shuffles, exposure and gamma changes, Gaussian noise, and texture based occlusions. Augmentation generated one additional mask image, the occluded mask, which was used in the loss function to aid occlusion detection.

To visualize the generated geometry mappings, we implemented four different visualizations: texture projection, inverse texture projection, grid lines projection, and grid points projection. These projections were used to assess the accuracy and stability of the geometry mapping, both in static images and in videos. In addition, we created a custom browser-based results evaluation page that used these visualization images and other illustrations to help assess the performance of the network.

When given real-world human faces, our final \ac{FCNN} was successful in doing the segmentation and geometry mapping, even though it had been trained on non-realistic synthetic data. The segmenting was, in most cases, good with sharp edges and geometry mapping worked well with non-extreme input images. Our data augmentation strategy successfully expanded the training dataset and prevented the network from overfitting. The occlusion augmentation method also enabled the network to inpaint geometry under obstructions that had not been in the training material, e.g., sunglasses or facial hair. When applied to a video, the temporal stability of the generated geometry was not bad but could be improved. Inpainted geometry was, most of the time, somewhat distorted and not temporally stable.

Future work could include 3D mesh generation from the UV mapping using, for example, morphable models and synthesis-by-analysis. Instead of doing a 2D-to-2D mapping and then 3D mesh reconstruction, it should be possible to train an \ac{FCNN} to do straight 2D-to-3D volumetric mapping. The temporal stability of the generated geometry was not perfect, especially under occlusions, and could be improved.
